from langchain_core.messages import AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from typing import Dict, Any

from app.core.state import AgentState
from app.services.llm_factory import llm_factory
from app.core.logger import logger

# åˆå§‹åŒ– LLM
llm = llm_factory.get_llm(mode="smart")

# å®šä¹‰ Prompt
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    
    ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
    {context}
    
    æ³¨æ„ï¼š
    1. å¦‚æœä¸Šä¸‹æ–‡åŒ…å«å›¾è°±å®ä½“å…³ç³»ï¼Œè¯·åœ¨å›ç­”ä¸­è‡ªç„¶ä½“ç°ã€‚
    2. ä¿æŒå›ç­”æ¡ç†æ¸…æ™°ã€‚"""),
    
    # âœ… è‡ªåŠ¨æ’å…¥å†å²æ¶ˆæ¯ (User + AI)
    MessagesPlaceholder(variable_name="messages"),
    
    ("user", "{question}")
])

# LCEL é“¾
rag_chain = rag_prompt | llm | StrOutputParser()

async def generation_node(state: AgentState) -> Dict[str, Any]:
    """
    ç”ŸæˆèŠ‚ç‚¹ï¼šç”Ÿæˆå›ç­”å¹¶æ›´æ–°å†å²
    """
    logger.info("ğŸ§  [ç”ŸæˆèŠ‚ç‚¹] æ­£åœ¨ç”Ÿæˆå›ç­”...")
    
    query = state["query"]
    context = state.get("rag_context", "")
    messages = state.get("messages", [])
    
    try:
        response_text = await rag_chain.ainvoke({
            "context": context,
            "messages": messages,
            "question": query
        })
        
        logger.success("âœ… å›ç­”ç”Ÿæˆå®Œæ¯•")
        
        return {
            "answer": response_text,
            "messages": [AIMessage(content=response_text)]
        }
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        return {
            "answer": "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚",
            "messages": [AIMessage(content="ç³»ç»Ÿé”™è¯¯")]
        }